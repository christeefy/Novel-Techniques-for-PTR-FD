{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Causality in Eastman Benchmark Data\n",
    "### using techniques developed so far including:\n",
    "- Probabilistic Graphical Models\n",
    "- Convergent Cross Mapping (CCM) algorithm\n",
    "- Cross Mapping Smoothness (CMS) algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import relevant libraries\n",
    "import numpy as np # For tensor manipulation in Python\n",
    "import pandas as pd # Store dataset as a Panda dataframe\n",
    "import csv # To load csv files\n",
    "\n",
    "# Interactive plotting\n",
    "import plotly\n",
    "import plotly.offline as pyo # For offline interactive plotting\n",
    "import plotly.graph_objs as go # For generating JSON plot objects in Plotly\n",
    "\n",
    "\n",
    "# Activate Plotly Offline for Jupyter\n",
    "pyo.init_notebook_mode(connected=True)\n",
    "\n",
    "# Set printing precision\n",
    "np.set_printoptions(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Definitions\n",
    "### Convergent Cross Mapping (CCM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0,
     11,
     30,
     53,
     90,
     335
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CCM(data, target, k, attractor_viz=False, prediction_corr_viz=False):\n",
    "    '''\n",
    "    Perform convergent cross-mapping (CCM) algorithm described in paper.\n",
    "    Inputs:\n",
    "        data:   Data to perform k-NN, a numpy array (N x P)\n",
    "        target: Target values to perform prediction (N x P)\n",
    "        k:      Number of nearest neighbours (scalar)\n",
    "    Returns:\n",
    "        predictions: Predicted values (1-D array)\n",
    "        causality:   Calculated causality from correlation plot (float)\n",
    "    '''\n",
    "    def euclidean_dist(A, B=None):\n",
    "        '''\n",
    "        Calculate the euclidean distance for rows in matrix A and rows in matrix B.\n",
    "        If B is None, calculates distances for rows between matrix A.\n",
    "        Inputs:\n",
    "            A: A matrix (a x P)\n",
    "            B: A matrix (b x k x P)\n",
    "        Returns:\n",
    "            A distance matrix (a x b), indicating the distance of all non-i-th point to the i-th point. \n",
    "        ''' \n",
    "        # Define input matrices with expanded dimensions\n",
    "        A_expanded = np.expand_dims(A, 2)\n",
    "        \n",
    "        # Calculate distance of each point and every other point\n",
    "        if B is None:\n",
    "            return np.sqrt(np.sum(np.square(A_expanded - np.transpose(A_expanded, (2, 1, 0))), axis=1))\n",
    "        else:\n",
    "            return np.sqrt(np.sum(np.square(np.transpose(A_expanded, (0,2,1)) - B), axis=2))\n",
    "   \n",
    "    def kNN(k, data):\n",
    "        '''\n",
    "        Return the nearest neighbours to each row in data in the form of a responsibility matrix.\n",
    "        Inputs:\n",
    "            k:    Number of nearest neighbours (scalar)\n",
    "            data: Data to perform k-NN, a numpy array (N x P)\n",
    "        Returns:\n",
    "            A responsibility matrix (N x k), listing the indices of the k-nearest neighbours for each row\n",
    "        '''\n",
    "\n",
    "        def responsibilities(k, distances):\n",
    "            '''\n",
    "            Finds the k-nearest neighbours to each point by index.\n",
    "            Inputs:\n",
    "                k:         Number of nearest neighbours (scalar)\n",
    "                distances: A distance matrix (N x N)\n",
    "            Returns:\n",
    "                A responsibility matrix (N x k), listing the indices of the k-nearest neighbours for each row\n",
    "            '''\n",
    "            return np.argsort(distances)[:,1:(k + 1)]\n",
    "\n",
    "        return responsibilities(k, euclidean_dist(data))\n",
    "\n",
    "    def predict_target(data, target, responsibilities):\n",
    "        '''\n",
    "        Performa a prediction of the target based on a weighting of contemporaneous neighbours of data.\n",
    "        Inputs:\n",
    "            data:             Data values (N x P)\n",
    "            target:           Target values to perform prediction (N x P)\n",
    "            responsibilities: A responsibility matrix (N x k)\n",
    "        Returns:\n",
    "            An array of predicted target values (N)\n",
    "        '''\n",
    "\n",
    "        def calculate_weights(data, responsibilities):\n",
    "            '''\n",
    "            Calculate weights based on the k-nearest neighbours\n",
    "            Inputs:\n",
    "                data:             Data values (N x P)\n",
    "                responsibilities: A responsibility matrix (N x k)\n",
    "            Returns:\n",
    "                A matrix of weights (N x k)\n",
    "            '''\n",
    "            # Obtain shape of responsibilities\n",
    "            N, k = responsibilities.shape\n",
    "\n",
    "            # Calculate values for numerator\n",
    "            for i in range(k):\n",
    "                numerator = np.exp( - np.divide(euclidean_dist(data, data[responsibilities]), \\\n",
    "                                                euclidean_dist(data, data[responsibilities])[:,0][:, np.newaxis]))\n",
    "\n",
    "            # Calculate denominator\n",
    "            denominator = np.sum(numerator, axis=1, keepdims=True)\n",
    "\n",
    "            # Calculate and return weights\n",
    "            return np.divide(numerator, denominator)\n",
    "        \n",
    "        weights = calculate_weights(data, responsibilities)\n",
    "        return np.sum(target[responsibilities] * np.expand_dims(weights, axis=2), axis=1)\n",
    "    \n",
    "    def visualise_attractor(data, target, responsibilities, predictions):\n",
    "        '''\n",
    "        Produce Plotly animation on a 1 x 2 subplot to visualise nearest neighbours of data and target.\n",
    "        Inputs:\n",
    "            data:        Data values (N x P)\n",
    "            target:      Target values (N x P)\n",
    "            responsibilities: A responsibility matrix (N x k)\n",
    "            predictions: Predicted values for target (N x P)\n",
    "        '''\n",
    "        fig = plotly.tools.make_subplots(rows=1, cols=2, specs=[[{'is_3d': True}, {'is_3d': True}]])\n",
    "        \n",
    "        # Define colour list\n",
    "        colour_list = np.array(['#b3b3b3', '#0f3957', '#1f77b4', '#ff7f0e'])\n",
    "\n",
    "        # Define blank figure\n",
    "        figure = {\n",
    "            'data': [],\n",
    "            'layout': {},\n",
    "            'frames': []\n",
    "        }\n",
    "\n",
    "        # Create layout\n",
    "        figure['layout'] = {\n",
    "            'width': 1000,\n",
    "            'height': 700,\n",
    "            'scene1': {\n",
    "                'domain': {\n",
    "                    'x': [0, 0.45],\n",
    "                    'y': [0., 1.]\n",
    "                }\n",
    "            },\n",
    "            'scene2': {\n",
    "                'domain': {\n",
    "                    'x': [0.55, 1.],\n",
    "                    'y': [0., 1.]\n",
    "                }\n",
    "            },\n",
    "            'title': 'Visualising Nearest Neighbours on Attractors',\n",
    "            'showlegend': False\n",
    "        }\n",
    "        \n",
    "        # Define buttons\n",
    "        figure['layout']['updatemenus'] = [\n",
    "            {\n",
    "                'buttons': [\n",
    "                    {\n",
    "                        'args': [None, {'frame': {'duration': 1000, 'redraw': False},\n",
    "                                 'fromcurrent': True, 'transition': {'duration': 0, 'easing': 'quadratic-in-out'}}],\n",
    "                        'label': 'Play',\n",
    "                        'method': 'animate'\n",
    "                    },\n",
    "                    {\n",
    "                        'args': [[None], {'frame': {'duration': 0, 'redraw': False}, 'mode': 'immediate',\n",
    "                        'transition': {'duration': 0}}],\n",
    "                        'label': 'Pause',\n",
    "                        'method': 'animate'\n",
    "                    }\n",
    "                ],\n",
    "                'direction': 'left',\n",
    "                'pad': {'r': 10, 't': 87},\n",
    "                'showactive': False,\n",
    "                'type': 'buttons',\n",
    "                'x': 0.1,\n",
    "                'xanchor': 'right',\n",
    "                'y': 0,\n",
    "                'yanchor': 'top'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Define slider dictionary\n",
    "        slider_dict = {\n",
    "            'active': 0, # Slider knob's relative starting location\n",
    "            'pad': {'b': 10, 't': 50}, # Bottom and top padding\n",
    "            'len': 0.9, # Slider length\n",
    "            'x': 0.1, # Slider x-position\n",
    "            'y': 0, # Slider y-position\n",
    "            'yanchor': 'top', \n",
    "            'xanchor': 'left',\n",
    "            'currentvalue': { # Displays current value selected by slider\n",
    "                'font': {'size': 20},\n",
    "                'prefix': 'Time index: ',\n",
    "                'visible': True,\n",
    "                'xanchor': 'right'\n",
    "            },\n",
    "            'transition': {'duration': 300, 'easing': 'cubic-in-out'},\n",
    "            'steps': []\n",
    "        }\n",
    "\n",
    "        # Create frames\n",
    "        for i in range(len(predictions)):\n",
    "            # Define a dictionary for each frame\n",
    "            frame = {\n",
    "                'data': [],\n",
    "                'name': str(i + 1) # Used to connect each frame to slider value\n",
    "            }\n",
    "            \n",
    "            # Create raw data trace\n",
    "            data_trace = {\n",
    "                'x': data[:,0],\n",
    "                'y': data[:,1],\n",
    "                'z': data[:,2],\n",
    "                'mode': 'markers',\n",
    "                'type': 'scatter3d',\n",
    "                'hoverinfo': 'none',\n",
    "                'scene': 'scene1',\n",
    "                'marker': {\n",
    "                    'size': 4,\n",
    "                    'color': colour_list[0]\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Create source point trace\n",
    "            source_trace = {\n",
    "                'x': [data[i,0]],\n",
    "                'y': [data[i,1]],\n",
    "                'z': [data[i,2]],\n",
    "                'mode': 'markers',\n",
    "                'type': 'scatter3d',\n",
    "                'name': 'Source',\n",
    "                'scene': 'scene1',\n",
    "                'hoverinfo': 'name',\n",
    "                'marker': {\n",
    "                    'size': 10,\n",
    "                    'symbol': 'diamond',\n",
    "                    'color': colour_list[2],\n",
    "                    'line': {'width': 1}\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Create source neighbours trace\n",
    "            source_neighbour_trace = {\n",
    "                'x': data[responsibilities[i,:],0],\n",
    "                'y': data[responsibilities[i,:],1],\n",
    "                'z': data[responsibilities[i,:],2],\n",
    "                'mode': 'markers',\n",
    "                'type': 'scatter3d',\n",
    "                'name': 'Source Neighbour',\n",
    "                'scene': 'scene1',\n",
    "                'hoverinfo': 'name',\n",
    "                'marker': {\n",
    "                    'size': 4,\n",
    "                    'color': colour_list[2],\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Create target trace\n",
    "            target_trace = {\n",
    "                'x': target[:,0],\n",
    "                'y': target[:,1],\n",
    "                'z': target[:,2],\n",
    "                'mode': 'markers',\n",
    "                'type': 'scatter3d',\n",
    "                'scene': 'scene2',\n",
    "                'hoverinfo': 'none',\n",
    "                'marker': {\n",
    "                    'size': 4,\n",
    "                    'color': colour_list[0]\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Create destination point trace\n",
    "            actual_destination_trace = {\n",
    "                'x': [target[i,0]],\n",
    "                'y': [target[i,1]],\n",
    "                'z': [target[i,2]],\n",
    "                'mode': 'markers',\n",
    "                'type': 'scatter3d',\n",
    "                'name': 'Actual Target',\n",
    "                'scene': 'scene2',\n",
    "                'hoverinfo': 'name',\n",
    "                'marker': {\n",
    "                    'size': 12,\n",
    "                    'symbol': 'diamond',\n",
    "                    'color': colour_list[3],\n",
    "                    'line': {'width': 2}\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Create destination neighbours trace\n",
    "            destination_neighbour_trace = {\n",
    "                'x': target[responsibilities[i,:],0],\n",
    "                'y': target[responsibilities[i,:],1],\n",
    "                'z': target[responsibilities[i,:],2],\n",
    "                'mode': 'markers',\n",
    "                'type': 'scatter3d',\n",
    "                'name': 'Target Neighbours',\n",
    "                'scene': 'scene2',\n",
    "                'hoverinfo': 'name',\n",
    "                'marker': {\n",
    "                    'size': 4,\n",
    "                    'color': colour_list[2],\n",
    "                }\n",
    "            } \n",
    "            \n",
    "            # Create predicted destination trace\n",
    "            predicted_destination_trace = {\n",
    "                'x': [predictions[i,0]],\n",
    "                'y': [predictions[i,1]],\n",
    "                'z': [predictions[i,2]],\n",
    "                'mode': 'markers',\n",
    "                'type': 'scatter3d',\n",
    "                'name': 'Predicted Target',\n",
    "                'scene': 'scene2',\n",
    "                'hoverinfo': 'name',\n",
    "                'marker': {\n",
    "                    'size': 10,\n",
    "                    'symbol': 'diamond',\n",
    "                    'color': colour_list[2],\n",
    "                    'line': {'width': 2}\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Append traces to frame\n",
    "            for trace in [data_trace, source_trace, source_neighbour_trace, \\\n",
    "                          target_trace, destination_neighbour_trace, \\\n",
    "                          actual_destination_trace, predicted_destination_trace]:\n",
    "                frame['data'].append(trace)\n",
    "            \n",
    "            # Append frame to figure\n",
    "            figure['frames'].append(frame)\n",
    "            \n",
    "            # Define slider step\n",
    "            slider_step = {\n",
    "                'args': [\n",
    "                    [i + 1],\n",
    "                    {'frame': {'duration': 300, 'redraw': False},\n",
    "                     'mode': 'immediate',\n",
    "                     'transition': {'duration': 0}}\n",
    "                ],\n",
    "                'label': i + 1,\n",
    "                'method': 'animate'\n",
    "            }\n",
    "            \n",
    "            # Append slider step to slider dictionary\n",
    "            slider_dict['steps'].append(slider_step)\n",
    "            \n",
    "        # Add sliders to layout\n",
    "        figure['layout']['sliders'] = [slider_dict]\n",
    "        \n",
    "        # Define figure['data']\n",
    "        figure['data'] = figure['frames'][0]['data']\n",
    "        \n",
    "        # Save snapshots locally\n",
    "        pyo.iplot(figure)\n",
    "    \n",
    "    def visualise_predictions(target, predictions):\n",
    "        '''\n",
    "        Create a scatterplot visualising predictions vs. target.\n",
    "        Inputs:\n",
    "            target:      Target values (N x P)\n",
    "            predictions: Prediction values (N x P)\n",
    "        '''\n",
    "        trace = go.Scatter(\n",
    "            x = target[:,-1],\n",
    "            y = predictions[:,-1],\n",
    "            mode = 'markers',\n",
    "        )\n",
    "        \n",
    "        line_trace = go.Scatter(\n",
    "            x = [0, 1],\n",
    "            y = [0, 1],\n",
    "            mode = 'lines',\n",
    "            hoverinfo = 'none',\n",
    "            line = {\n",
    "                'color': '#000000',\n",
    "                'dash': 'dash',\n",
    "                'width': 3\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        layout = go.Layout(\n",
    "            title = 'Correlation Plot (r = {})'\\\n",
    "                    .format(np.round(np.corrcoef(target[:,-1], predictions[:,-1])[0,-1], 3)),\n",
    "            showlegend = False,\n",
    "            height = 800,\n",
    "            width = 700,\n",
    "            xaxis = {'title': 'Target',},\n",
    "            yaxis = {'title': 'Prediction', 'scaleanchor': 'x'},\n",
    "        )\n",
    "        \n",
    "        figure = go.Figure(data=go.Data([trace, line_trace]), layout=layout)\n",
    "        pyo.iplot(figure)\n",
    "        \n",
    "    \n",
    "    ###################\n",
    "    # Function begins #\n",
    "    ###################\n",
    "    \n",
    "    # Find indices of k-nearest neighbours\n",
    "    responsibilities = kNN(k, data)\n",
    "    print 'k-NN complete!'\n",
    "    \n",
    "    # Calculate predicted target values\n",
    "    predictions = predict_target(data, target, responsibilities)\n",
    "    print 'Predictions complete!'\n",
    "    \n",
    "    # Create interactive attractor animation\n",
    "    if attractor_viz == True:\n",
    "        visualise_attractor(data, target, responsibilities, predictions)\n",
    "        \n",
    "    # Create correlation plot\n",
    "    if prediction_corr_viz == True:\n",
    "        visualise_predictions(target, predictions)\n",
    "        \n",
    "    # Calculate causality based on correlation\n",
    "    causality = np.round(np.corrcoef(target[:,-1], predictions[:,-1])[0,-1], 3)\n",
    "    \n",
    "    return predictions, causality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Mapping Smoothness (CMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0,
     13,
     24,
     34,
     55,
     79,
     94,
     123,
     140,
     247,
     290
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CMS_mod(data, target, coupling_parameter=None, scope=20, \\\n",
    "                   show_clusters=False, show_corr_plot=False, show_causality_time_series=False):\n",
    "    '''\n",
    "    Calculate the causality index of data -> target.\n",
    "    Inputs:\n",
    "        data:               Input values (N x P) (suspected 'effect' variable)\n",
    "        target:             Output values (N x P)(suspected 'cause' variable)\n",
    "        scope:              Length of time series subset to perform causality calculations (int)\n",
    "        show_clusters: If True, scatter plot of clusters will be shown for first set of calculation (Boolean)\n",
    "    Returns:\n",
    "        An array of normalised causality index (N - scope)\n",
    "    '''\n",
    "    \n",
    "    def l2(A, axis=None):\n",
    "        '''\n",
    "        Calculates the L2-norm of a tensor, at a specified axis.\n",
    "        Inputs:\n",
    "            A:    A tensor.\n",
    "            axis: Summation axis.\n",
    "        Returns:\n",
    "            L2-norm of tensor.\n",
    "        '''\n",
    "        return np.sqrt(np.sum(np.square(A), axis=axis))\n",
    "    \n",
    "    def RMS(A):\n",
    "        '''\n",
    "        Perform a root-mean square operation.\n",
    "        Input:\n",
    "            A: A 1-D array (N)\n",
    "        Returns:\n",
    "            Root means square of A.\n",
    "        '''\n",
    "        return np.sqrt(np.mean(np.square(A)))\n",
    "    \n",
    "    def euclidean_dist(A, B=None):\n",
    "        '''\n",
    "        Calculate the euclidean distance for rows in matrix A and rows in matrix B.\n",
    "        If B is None, calculates distances for rows between matrix A.\n",
    "        Inputs:\n",
    "            A: A matrix (a x P)\n",
    "            B: A matrix (b x P)\n",
    "        Returns:\n",
    "            A distance matrix (a x b), indicating the distance of all non-i-th point to the i-th point. \n",
    "        ''' \n",
    "        # Define input matrices with expanded dimensions\n",
    "        A_expanded = np.expand_dims(A, 2)\n",
    "\n",
    "        # Calculate distance of each point and every other point\n",
    "        if B is None:\n",
    "            B_expanded = A_expanded\n",
    "        else:\n",
    "            B_expanded = np.expand_dims(B, 2)\n",
    "\n",
    "        return l2(A_expanded - np.transpose(B_expanded, (2, 1, 0)), axis=1)\n",
    "\n",
    "    def find_cluster_params(data):\n",
    "        '''\n",
    "        Given datapoints, find centres and variances for each radial basis.\n",
    "        Assumptions:\n",
    "            Each datapoint is itself a radial basis function.\n",
    "            Variances are assumed to be identical for all basis functions. Calculated as four times the\n",
    "                average L2 squared distance between all pairwise datapoints.\n",
    "        Inputs:\n",
    "            data: Data values (N x P)\n",
    "        Returns:\n",
    "            centres:  Centre coordinates for each radial basis (N x P)\n",
    "            variance: Variance of radial basis (scalar)\n",
    "        '''\n",
    "        # Calculate Euclidean distance matrix (N x N)\n",
    "        distances = euclidean_dist(data)\n",
    "\n",
    "        # Obtain upper triangular of distances (excluding diagonals)\n",
    "        distances[np.triu_indices(distances.shape[0], 0)] = 0\n",
    "\n",
    "        # Calculate variance as four times of [average(Euclidean distance)]^2\n",
    "        variance = 0.001 *(np.mean(distances) * np.true_divide(np.size(distances), np.sum(distances != 0)))**2\n",
    "\n",
    "        return data, variance\n",
    "\n",
    "    def calc_radial_basis_activations(data, centres, variances):\n",
    "        '''\n",
    "        Calculates the activations for the hidden radial basis layer.\n",
    "        Inputs:\n",
    "            data: Data values (N x P)\n",
    "            centres: RBF centres (K x P)\n",
    "            variances: RBF variances (scalar)\n",
    "        Returns:\n",
    "            Radial basis activations (N x K)\n",
    "        '''\n",
    "        # Calculate Gaussian exponent\n",
    "        actv = np.exp( - np.divide(euclidean_dist(data, centres)**2, 2 * np.transpose(variances)))\n",
    "            \n",
    "        return actv\n",
    "\n",
    "    def train_RBFN_weights(data, target, centres, variances):\n",
    "        '''\n",
    "        Train the weights of a Radial Basis Function Network by solving for values in parameter \\mathbf{\\alpha}.\n",
    "        Normalise the activations before solving for \\mathbf{\\alpha}, \n",
    "            and re-normalising the values of \\mathbf{\\alpha} after.\n",
    "\n",
    "        Inputs:\n",
    "            data:   Data values (N x P)\n",
    "            target: Target values (N x P)\n",
    "            centres: Cluster centres (K x P)\n",
    "            variances: Cluster variances (K)\n",
    "        Returns:\n",
    "            Trained weights, \\mathbf{\\alpha} (K x P)\n",
    "        '''\n",
    "        # Obtain activations\n",
    "        activations = calc_radial_basis_activations(data, centres, variances)\n",
    "        \n",
    "        # Store the L2-norms of columns of activations in a matrix\n",
    "        L2 = l2(activations, axis=0)\n",
    "        \n",
    "        # Normalise activation values by their L2-norms\n",
    "        actvn_norm = np.divide(activations, np.expand_dims(L2, axis=0))\n",
    "        \n",
    "        # Solve system of linear equations for alpha\n",
    "        weights = np.linalg.solve(actvn_norm, target)\n",
    "\n",
    "        # Return re-normalised alpha\n",
    "        return np.divide(weights, np.expand_dims(L2, axis=1))\n",
    "\n",
    "    def RBFN_calc(data, centres, variances, weights):\n",
    "        '''\n",
    "        Calculate the output of the trained RBFN.\n",
    "        Inputs:\n",
    "            data:      Data values (N x P)\n",
    "            centres:   RBF centres (K x P)\n",
    "            variances: RBF variances (scalar)\n",
    "            weights:   Trained weights (K x P)\n",
    "        Returns:\n",
    "            Predicted target value (N x P), assuming data and target have same dimensionality\n",
    "        '''\n",
    "        # Calculate radial basis activations\n",
    "        actv = calc_radial_basis_activations(data, centres, variances)\n",
    "        \n",
    "        # Calculate and return predicted output\n",
    "        return np.matmul(actv, weights)\n",
    "    \n",
    "    def visualise_clusters(data, centres, variances):\n",
    "        '''\n",
    "        Final result by colouring data points by clusters generated by Mixture of Gaussian algorithm\n",
    "        Inputs:\n",
    "            data:      Data points (N x P)\n",
    "            centres:   Ccoordinates of cluster centres (K x P)\n",
    "            variances: Cluster variances (float)\n",
    "            \n",
    "        '''\n",
    "        def calc_ellipse_coordinates(centres, variances):\n",
    "            '''\n",
    "            Create x- and y-coordinates for ellipses for each cluster\n",
    "            Assumptions:\n",
    "                Dimension of data point is 2\n",
    "            Returns:\n",
    "                ellipse: x- and y-coordinates for K ellipses (N x K x D)\n",
    "            '''\n",
    "            # Create trace for region to encompass 95% of the points (using Chi-squared critical value)\n",
    "            # Assuming joint independence and equal marginal variances\n",
    "\n",
    "            # Chi-squared with df 2 and alpha=5%\n",
    "            crit_val = 5.991\n",
    "\n",
    "            # Calculate axes length\n",
    "            axis_lengths = np.sqrt(variances * crit_val)\n",
    "\n",
    "            # Calculate coordinates to trace ellipse\n",
    "            t = np.arange(-np.pi, np.pi + np.pi / 50, np.pi / 50) # Parameter\n",
    "            x = np.transpose(centres[:,0][:, np.newaxis]) + axis_lengths * np.cos(t)[:, np.newaxis]\n",
    "            y = np.transpose(centres[:,1][:, np.newaxis]) + axis_lengths * np.sin(t)[:, np.newaxis]\n",
    "\n",
    "            # Stack x- and y-coordinates along axis=2\n",
    "            ellipse = np.stack([x, y], axis=2)\n",
    "\n",
    "            return ellipse\n",
    "\n",
    "        #######################\n",
    "        ##  Function begins  ##\n",
    "        #######################\n",
    "\n",
    "        # Create ellipse coordinates\n",
    "        ellipse = calc_ellipse_coordinates(centres, variances)\n",
    "\n",
    "        # Define colour list as per Plotly's default colour list\n",
    "        colour_list = np.array(['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b'])\n",
    "\n",
    "        # Define blank figure\n",
    "        figure = {\n",
    "            'data': [],\n",
    "            'layout': {}\n",
    "        }\n",
    "        \n",
    "        # Create data trace\n",
    "        data_trace = {\n",
    "            'x': np.round(data[:, 0], 3),\n",
    "            'y': np.round(data[:, 1], 3),\n",
    "            'mode': 'markers',\n",
    "            'hoverinfo': 'none',\n",
    "            'marker': {\n",
    "                'color': '#d3d3d3'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        figure['data'].append(data_trace)\n",
    "\n",
    "        for k in range(len(centres)):\n",
    "            # Create trace for cluster centres\n",
    "            centre_trace = {\n",
    "                'x': np.round([centres[k][0]], 3),\n",
    "                'y': np.round([centres[k][1]], 3),\n",
    "                'hoverinfo': 'none',\n",
    "                'mode': 'markers',\n",
    "                'marker': {\n",
    "                        'size': 12,\n",
    "                        'symbol': 'diamond',\n",
    "                        'color': colour_list[0],\n",
    "                        'line': {'width': 3}\n",
    "                    }   \n",
    "            }\n",
    "\n",
    "            # Create trace for region encompassing 95% of data points\n",
    "            variance_trace = {\n",
    "                'x': ellipse[:,k,:][:,0],\n",
    "                'y': ellipse[:,k,:][:,1],\n",
    "                'hoverinfo': 'none',\n",
    "                'mode': 'lines',\n",
    "                'marker': {\n",
    "                    'color': colour_list[0]\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Add cluster trace\n",
    "            for trace in [centre_trace, variance_trace]:\n",
    "                figure['data'].append(trace)\n",
    "\n",
    "        # Generate figure layout\n",
    "        figure['layout'] = go.Layout(\n",
    "            width = 900,\n",
    "            height = 900,\n",
    "            showlegend = False,\n",
    "            title = 'Visualisation of First {} Clusters'.format(len(centres)),\n",
    "            xaxis = {'autorange': True},\n",
    "            yaxis = {'autorange': True, 'scaleanchor': 'x'}\n",
    "        )\n",
    "\n",
    "        return pyo.iplot(figure)\n",
    "    \n",
    "    def correlation_plot(target, prediction):\n",
    "        '''\n",
    "        Produce a correlation plot between latest time slices of target and prediction.\n",
    "        Input:\n",
    "            target:     Target values (N x P)\n",
    "            prediction: Predicted target values (N x P)\n",
    "        '''\n",
    "        # Create trace for points to plot\n",
    "        point_trace = go.Scatter(\n",
    "            x = target[:,-1],\n",
    "            y = prediction[:,-1],\n",
    "            mode = 'markers',\n",
    "        )\n",
    "        \n",
    "        print target.shape\n",
    "        print prediction.shape\n",
    "        \n",
    "        # Create a line trace for plotting a 45deg line\n",
    "        line_trace = go.Scatter(\n",
    "            x = [np.min(target, axis=-1), np.max(target, axis=-1)],\n",
    "            y = [np.min(target, axis=-1), np.max(target, axis=-1)],\n",
    "            mode = 'lines',\n",
    "            line = {\n",
    "                'dash': 'dash'\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Define layout\n",
    "        layout = go.Layout(\n",
    "            width = 800,\n",
    "            height = 900,\n",
    "            showlegend = False,\n",
    "            xaxis = { 'title': 'Target value' },\n",
    "            yaxis = { 'title': 'Predicted target value', 'scaleanchor': 'x' },\n",
    "            title = 'Correlation Plot between Target & Predicted Values'\n",
    "        )\n",
    "        \n",
    "        # Define figure\n",
    "        figure = go.Figure(data=go.Data([point_trace, line_trace]), layout=layout)\n",
    "        \n",
    "        # Plot figure offline\n",
    "        pyo.iplot(figure)\n",
    "    \n",
    "    def casuality_time_series(causality, coupling_parameter):\n",
    "        '''\n",
    "        Create a time series plot of the causality indices.\n",
    "        Inputs:\n",
    "            causality: Causality index\n",
    "        '''\n",
    "        # Generate empty figure\n",
    "        figure = {\n",
    "            'data': [],\n",
    "            'layout': {},\n",
    "        }\n",
    "        \n",
    "        # Define colour palette\n",
    "        colour_list = np.array(['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b'])\n",
    "        \n",
    "        # Define trace\n",
    "        trace = {\n",
    "            'x': range(len(causality)),\n",
    "            'y': causality,\n",
    "            'mode': 'lines',\n",
    "            'name': 'Causality Index',\n",
    "        }\n",
    "        # Append trace\n",
    "        figure['data'].append(trace)\n",
    "        \n",
    "        # Define layout\n",
    "        figure['layout'] = {\n",
    "            'title': 'Time Series Plot of Causality Index',\n",
    "            'xaxis': {'title': 'Time Index'},\n",
    "            'yaxis': {\n",
    "                'title': 'Causality Index',\n",
    "                'titlefont': { 'color': colour_list[0] },\n",
    "                'tickfont': { 'color': colour_list[0] }, \n",
    "                'range': [0, 1],\n",
    "            },\n",
    "            'showlegend': False,\n",
    "        }\n",
    "        \n",
    "        # Define trace for coupling parameter\n",
    "        if coupling_parameter is not None:\n",
    "            # Plot the last len(causality) values\n",
    "            param_trace = {\n",
    "                'x': range(len(causality)),\n",
    "                'y': coupling_parameter[-len(causality):],\n",
    "                'mode': 'lines',\n",
    "                'name': 'Coupling Parameter',\n",
    "                'yaxis': 'y2',\n",
    "                'line': { 'dash': 'dash' }\n",
    "            }\n",
    "            \n",
    "            # Append trace\n",
    "            figure['data'].append(param_trace)\n",
    "            \n",
    "            # Add secondary axis to layout\n",
    "            figure['layout']['yaxis2'] = {\n",
    "                'title': 'Coupling parameter',\n",
    "                'titlefont': { 'color': colour_list[1] },\n",
    "                'tickfont': { 'color': colour_list[1] },\n",
    "                'range': [0, np.max(coupling_parameter) * 1.1],\n",
    "                'overlaying': 'y',\n",
    "                'side': 'right',\n",
    "                'showgrid': False,\n",
    "            }\n",
    "        \n",
    "        # Display figure\n",
    "        pyo.iplot(figure)\n",
    "        \n",
    "    \n",
    "    ###################\n",
    "    # Function Begins #\n",
    "    ###################\n",
    "    \n",
    "    # Error checking\n",
    "    try:\n",
    "        assert data.shape[0] >= scope\n",
    "    except:\n",
    "        print 'Error: Time series is shorter than scope. Please ensure scope is not shorter than the length of your time series.'\n",
    "    \n",
    "    # Define variables\n",
    "    N = data.shape[0]\n",
    "    causality = np.zeros(N - scope + 1)\n",
    "    \n",
    "    # Visualise clusters to see if centres and variances are appropriate\n",
    "    if (show_clusters == True):\n",
    "        centres, variances = find_cluster_params(data=data[:scope]) \n",
    "        visualise_clusters(data, centres, variances)\n",
    "\n",
    "    print 'Calculating causality indices...'\n",
    "    \n",
    "    for i in range(N - scope + 1):\n",
    "        # Define subset of working data and target\n",
    "        scoped_data = data[i:(scope + i),:]\n",
    "        scoped_target = target[i:(scope + i),:]\n",
    "        \n",
    "        # Define error variable\n",
    "        error = np.zeros(scope)\n",
    "        \n",
    "        for j in range(scope):\n",
    "            # Obtain centres and variances of RBFs using leave-one-out scoped dataset\n",
    "            centres, variances = find_cluster_params(data=np.delete(scoped_data, j, 0))\n",
    "            \n",
    "            # Train weights of RBFN using leave-one-out scoped dataset\n",
    "            weights = train_RBFN_weights(\n",
    "                data = np.delete(scoped_data, j, 0), \n",
    "                target = np.delete(scoped_target, j, 0),\n",
    "                centres = centres,\n",
    "                variances = variances\n",
    "            )\n",
    "            \n",
    "            # Calculate predicted value of output\n",
    "            prediction = RBFN_calc(\n",
    "                data = scoped_data[j,:][np.newaxis, :],\n",
    "                centres = centres,\n",
    "                variances = variances,\n",
    "                weights = weights\n",
    "            )\n",
    "            \n",
    "            # Calculate error\n",
    "            error[j] = l2(prediction - scoped_target[j,:])\n",
    "        \n",
    "        # Calculate \\delta\n",
    "        delta = RMS(error) / RMS(l2(scoped_data - np.mean(scoped_data, axis=0), axis=1))\n",
    "        \n",
    "        # Calculate causality index\n",
    "        causality[i] = np.exp( - delta / 5.)\n",
    "        \n",
    "    print 'Causality calculations complete!'\n",
    "    \n",
    "    # If True, display a correlation plot\n",
    "    if show_corr_plot == True:\n",
    "        correlation_plot(target, prediction)\n",
    "        \n",
    "    # If True, display a time series of calculated causality\n",
    "    if show_causality_time_series == True:\n",
    "        casuality_time_series(causality, coupling_parameter)\n",
    "        \n",
    "    return causality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_delayed_vector(data, embed_dim, delay=1):\n",
    "    '''\n",
    "    Generate a delayed embedding vector of the data.\n",
    "    Input:\n",
    "        data:      A dictionary\n",
    "        embed_dim: Embedding dimensions (int)\n",
    "        delay:     Number of samples between each time series point (int)\n",
    "    '''\n",
    "    \n",
    "    assert embed_dim > 1\n",
    "    assert delay >= 1\n",
    "\n",
    "    # Get length from random key in dictionary\n",
    "    N = len(data[np.random.choice(list(data))])\n",
    "    N_embed = N - (embed_dim - 1) * delay\n",
    "    \n",
    "    # Generate empty storage variable\n",
    "    data_embed = {}\n",
    "    \n",
    "    # Initialize empty array for each key\n",
    "    for key in data.keys():\n",
    "        data_embed[key] = np.zeros((N_embed, embed_dim))\n",
    "        # Generate phase-shifted vectors for each key\n",
    "        for i in range(embed_dim):\n",
    "            data_embed[key][:,i] = data[key][(i * delay):(N_embed + i * delay)]\n",
    "        \n",
    "    return data_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualise_data(data, ex):\n",
    "    '''\n",
    "    Visualise data through an interactive graph.\n",
    "\n",
    "    Input:\n",
    "        data: Dictionary of numpy time-series variables in order of X, Y, Z, v1 and v2\n",
    "        ex: Example being plotted (1, 2, 3) (string)\n",
    "\n",
    "    Returns:\n",
    "        Interactive offline Plotly chart\n",
    "    '''\n",
    "    # Define empty trace and variable name list\n",
    "    traces = []\n",
    "\n",
    "    # Define and append trace for each variable\n",
    "    # Set visibility of noise variables to 'legendonly'\n",
    "    for key in sorted(data):\n",
    "        traces.append(\n",
    "            go.Scatter(\n",
    "                x = range(len(data[key])),\n",
    "                y = data[key],\n",
    "                name = key,\n",
    "                visible = 'legendonly' if key == 'v1' or key == 'v2' else True\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Convert traces to JSON-like object\n",
    "    data = go.Data(traces)\n",
    "\n",
    "    # Define JSON-like layout\n",
    "    layout = go.Layout(\n",
    "        title = 'Time-series Plots of Example ' + str(ex),\n",
    "        xaxis = {'title': 'Sampling time'},\n",
    "    )\n",
    "\n",
    "    # Generate and plot figure\n",
    "    figure = go.Figure(data=data, layout=layout)\n",
    "    pyo.iplot(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collapse_data(data, deg, ex, burn_in=0):\n",
    "    '''\n",
    "    Given degree d, collapse an N x M time-series data matrix into a (N - d + 1) x (d x M) data matrix. \n",
    "    Additional (d - 1) x M columns are for 'initial', ('intermediary') and 'final' values of all M variables.\n",
    "\n",
    "    Inputs:\n",
    "        data:    Matrix of M time-series each of length N (N x M)\n",
    "        d:       Degree of Bayesian network\n",
    "        ex:      Example being processed (1, 2, 3) (string)\n",
    "        burn_in: Initial number of sampling intervals ignored to ensure stationarity of time-series\n",
    "\n",
    "    Returns:\n",
    "        Collapsed data matrix of size (N - d + 1) x (d x M)\n",
    "        Headers of collapsed matrix\n",
    "    '''\n",
    "    # Define N and M\n",
    "    N, M = data.shape\n",
    "    \n",
    "    # Define empty output matrix\n",
    "    new_mat = np.zeros((N - deg + 1, deg * M))\n",
    "    \n",
    "    # Define header key\n",
    "    head_key = ['r1_', 'r2_', 'y1_', 'y2_'] if (str(ex) == '3' or str(ex) == '3_mod') else ['X', 'Y', 'Z']\n",
    "    \n",
    "    # Define empty header list\n",
    "    header = []\n",
    "    \n",
    "    # Create d duplicates of each time-series, and shift duplicated time-series by one sampling interval\n",
    "    # Populate header list\n",
    "    for col, var in enumerate(data.T):\n",
    "        for i in range(deg):\n",
    "            new_mat[:, col * deg + i] = var[i:(N - deg + i + 1)]\n",
    "            header.append(head_key[col] + str(i))\n",
    "    \n",
    "    return new_mat[max(burn_in - 1, 0):], header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_csv(df, deg, burn_in=0):\n",
    "    '''\n",
    "    Output data into CSV file.\n",
    "\n",
    "    Input:\n",
    "        df:      Panda dataframe\n",
    "        deg:     Degree of Bayesian network\n",
    "        burn_in: Initial number of sampling intervals ignored to ensure stationarity of time-series\n",
    "    '''\n",
    "    # Obtain rows of dataframe\n",
    "    N = df.shape[0]\n",
    "    \n",
    "    try:\n",
    "        assert N > burn_in\n",
    "    except:\n",
    "        print 'Burn_in value exceeds length of dataframe.'\n",
    "    \n",
    "    # Define empty dataframe\n",
    "    output = pd.DataFrame()\n",
    "    \n",
    "    # Create d duplicates of each time-series, and shift duplicated time-series by one sampling interval\n",
    "    for field, series in df.iteritems():\n",
    "        for i in range(deg):\n",
    "            output = pd.concat([output, series.shift(i).rename(field + '_' + str(i))], axis=1)\n",
    "    \n",
    "    # Removes rows containing NaN \n",
    "    # Reset index to start from 0\n",
    "    # Drop first burn_in rows\n",
    "    # Output to csv\n",
    "    (output\n",
    "     .dropna()\n",
    "     .reset_index(drop=True)\n",
    "     .iloc[burn_in:]\n",
    "     .to_csv(path_or_buf='./Data/Bayesian Network/eastman_deg{}.csv'.format(deg), index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Eastman Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1.PV</th>\n",
       "      <th>FC3.PV</th>\n",
       "      <th>LC1.PV</th>\n",
       "      <th>FC1.PV</th>\n",
       "      <th>FC4.PV</th>\n",
       "      <th>TC1.PV</th>\n",
       "      <th>FC6.PV</th>\n",
       "      <th>PC2.PV</th>\n",
       "      <th>LC3.PV</th>\n",
       "      <th>FC5.PV</th>\n",
       "      <th>...</th>\n",
       "      <th>TI8.PV</th>\n",
       "      <th>TI7.PV</th>\n",
       "      <th>PI2.PV</th>\n",
       "      <th>FI3.PV</th>\n",
       "      <th>LI1.PV</th>\n",
       "      <th>FC3.SP</th>\n",
       "      <th>FC1.SP</th>\n",
       "      <th>FC6.SP</th>\n",
       "      <th>FC5.SP</th>\n",
       "      <th>FC8.SP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.2034</td>\n",
       "      <td>6411.2</td>\n",
       "      <td>55.535</td>\n",
       "      <td>7.6151</td>\n",
       "      <td>13.420</td>\n",
       "      <td>108.38</td>\n",
       "      <td>1874.6</td>\n",
       "      <td>7.6552</td>\n",
       "      <td>42.375</td>\n",
       "      <td>4.7302</td>\n",
       "      <td>...</td>\n",
       "      <td>88.204</td>\n",
       "      <td>91.54</td>\n",
       "      <td>1.5884</td>\n",
       "      <td>2.4994</td>\n",
       "      <td>59.577</td>\n",
       "      <td>6393.9</td>\n",
       "      <td>7.5868</td>\n",
       "      <td>1888.3</td>\n",
       "      <td>4.6892</td>\n",
       "      <td>0.87133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.2061</td>\n",
       "      <td>6372.4</td>\n",
       "      <td>55.247</td>\n",
       "      <td>7.6119</td>\n",
       "      <td>13.307</td>\n",
       "      <td>108.43</td>\n",
       "      <td>1838.6</td>\n",
       "      <td>7.7258</td>\n",
       "      <td>41.479</td>\n",
       "      <td>4.7358</td>\n",
       "      <td>...</td>\n",
       "      <td>88.204</td>\n",
       "      <td>91.54</td>\n",
       "      <td>1.5884</td>\n",
       "      <td>2.5317</td>\n",
       "      <td>59.577</td>\n",
       "      <td>6391.5</td>\n",
       "      <td>7.6717</td>\n",
       "      <td>1806.5</td>\n",
       "      <td>4.6809</td>\n",
       "      <td>0.87171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.2256</td>\n",
       "      <td>6390.0</td>\n",
       "      <td>55.431</td>\n",
       "      <td>7.6151</td>\n",
       "      <td>13.302</td>\n",
       "      <td>108.47</td>\n",
       "      <td>1834.1</td>\n",
       "      <td>7.7018</td>\n",
       "      <td>41.556</td>\n",
       "      <td>4.7358</td>\n",
       "      <td>...</td>\n",
       "      <td>88.204</td>\n",
       "      <td>91.54</td>\n",
       "      <td>1.5946</td>\n",
       "      <td>2.5463</td>\n",
       "      <td>59.546</td>\n",
       "      <td>6372.4</td>\n",
       "      <td>7.6038</td>\n",
       "      <td>1809.1</td>\n",
       "      <td>4.6725</td>\n",
       "      <td>0.87196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.2228</td>\n",
       "      <td>6370.3</td>\n",
       "      <td>55.219</td>\n",
       "      <td>7.6055</td>\n",
       "      <td>13.430</td>\n",
       "      <td>108.51</td>\n",
       "      <td>1851.4</td>\n",
       "      <td>7.6952</td>\n",
       "      <td>41.958</td>\n",
       "      <td>4.7136</td>\n",
       "      <td>...</td>\n",
       "      <td>88.204</td>\n",
       "      <td>91.54</td>\n",
       "      <td>1.6103</td>\n",
       "      <td>2.5168</td>\n",
       "      <td>59.546</td>\n",
       "      <td>6374.8</td>\n",
       "      <td>7.6717</td>\n",
       "      <td>1847.4</td>\n",
       "      <td>4.6649</td>\n",
       "      <td>0.86945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.2089</td>\n",
       "      <td>6380.1</td>\n",
       "      <td>55.195</td>\n",
       "      <td>7.6151</td>\n",
       "      <td>13.323</td>\n",
       "      <td>108.59</td>\n",
       "      <td>1838.0</td>\n",
       "      <td>7.7435</td>\n",
       "      <td>41.826</td>\n",
       "      <td>4.7136</td>\n",
       "      <td>...</td>\n",
       "      <td>88.246</td>\n",
       "      <td>91.59</td>\n",
       "      <td>1.5978</td>\n",
       "      <td>2.5175</td>\n",
       "      <td>59.515</td>\n",
       "      <td>6384.3</td>\n",
       "      <td>7.6887</td>\n",
       "      <td>1837.2</td>\n",
       "      <td>4.6566</td>\n",
       "      <td>0.86970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PC1.PV  FC3.PV  LC1.PV  FC1.PV  FC4.PV  TC1.PV  FC6.PV  PC2.PV  LC3.PV  \\\n",
       "0  4.2034  6411.2  55.535  7.6151  13.420  108.38  1874.6  7.6552  42.375   \n",
       "1  4.2061  6372.4  55.247  7.6119  13.307  108.43  1838.6  7.7258  41.479   \n",
       "2  4.2256  6390.0  55.431  7.6151  13.302  108.47  1834.1  7.7018  41.556   \n",
       "3  4.2228  6370.3  55.219  7.6055  13.430  108.51  1851.4  7.6952  41.958   \n",
       "4  4.2089  6380.1  55.195  7.6151  13.323  108.59  1838.0  7.7435  41.826   \n",
       "\n",
       "   FC5.PV   ...     TI8.PV  TI7.PV  PI2.PV  FI3.PV  LI1.PV  FC3.SP  FC1.SP  \\\n",
       "0  4.7302   ...     88.204   91.54  1.5884  2.4994  59.577  6393.9  7.5868   \n",
       "1  4.7358   ...     88.204   91.54  1.5884  2.5317  59.577  6391.5  7.6717   \n",
       "2  4.7358   ...     88.204   91.54  1.5946  2.5463  59.546  6372.4  7.6038   \n",
       "3  4.7136   ...     88.204   91.54  1.6103  2.5168  59.546  6374.8  7.6717   \n",
       "4  4.7136   ...     88.246   91.59  1.5978  2.5175  59.515  6384.3  7.6887   \n",
       "\n",
       "   FC6.SP  FC5.SP   FC8.SP  \n",
       "0  1888.3  4.6892  0.87133  \n",
       "1  1806.5  4.6809  0.87171  \n",
       "2  1809.1  4.6725  0.87196  \n",
       "3  1847.4  4.6649  0.86945  \n",
       "4  1837.2  4.6566  0.86970  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eastman = pd.read_csv('./Data/eastman.csv')\n",
    "eastman.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Phase-Shifted Eastman Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv(eastman, 2, 0);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
