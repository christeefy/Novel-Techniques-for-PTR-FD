{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3.2: Mixture of Gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.2.0: Package initialisations, environment configuration and function definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# TensorFlow embedding API library\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "# Non-interactive plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Interactive plotting\n",
    "from plotly import tools\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as pyo\n",
    "from plotly.offline import download_plotlyjs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "# Global Variables\n",
    "CURRENT_DIR = '/Users/christophertee/Dropbox/University/MASc/Courses/Winter 2017' + \\\n",
    "              '/ECE521 (Inference Algorithms & Machine Learning)/Assignment 3'\n",
    "LOG_DIR = '/Logs'\n",
    "\n",
    "# Activate Plotly Offline for Jupyter\n",
    "pyo.init_notebook_mode(connected=True)\n",
    "\n",
    "# Define global variable SEED\n",
    "SEED = 521"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data2D.npy and data100D.npy into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data2D.npy contains 10,000 data points of dimension 2\n",
    "data100D.npy contains 10,000 data points of dimension 100\n",
    "\"\"\"\n",
    "# Load data\n",
    "data2D = np.load(\"./Data/RBFN/source_ex4.npz\")\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(521)\n",
    "\n",
    "# Generate random index\n",
    "randIdx2D = np.arange(len(data2D['Y']))\n",
    "\n",
    "# Randomise data2D\n",
    "np.random.shuffle(randIdx2D)\n",
    "data = data2D['Y'][randIdx2D]\n",
    "target = data2D['X'][randIdx2D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load results (Optional; when working resuming work session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# results_2_2_2 = np.load('./Results/MoG/2_2_2.npy')\n",
    "# results_2_2_3 = np.load('./Results/MoG/2_2_3.npy')\n",
    "# results_2_2_4_MoG = np.load('./Results/MoG/2_2_4_MoG.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Mixture of Gaussian (MoG) TensorFlow graph:\n",
    "\n",
    "### Loss function:\n",
    "\n",
    "$$ \\mathcal{L}(\\mathbf{\\mu}, \\mathbf{\\sigma}, \\mathbf{\\pi}) = \\prod_{i=1}^B \\sum_{k=1}^K \\pi_k \\mathcal{N} (\\mathbf{x}_n \\ | \\ \\mathbf{\\mu}_k, \\sigma_k^2) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     12
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Builds TensorFlow graph for MoG\n",
    "\n",
    "Input:\n",
    "    K: number of clusters\n",
    "    D: dimension of data (only 2 or 100 allowed)\n",
    "Internal variables:\n",
    "    X: input data matrix (N x D)\n",
    "    Mu: cluster centres (K x D)\n",
    "    sigma_sq: cluster variance (K x 1)\n",
    "    log_pi: log of latent cluster variables (K x 1)\n",
    "'''\n",
    "def build_MoG(K, D, device='cpu'):\n",
    "    '''\n",
    "    Calculate log probability density function for all pairs of B data points and K clusters\n",
    "\n",
    "    Assumptions:\n",
    "        Dimensions are independent and have the same standard deviation, sigma\n",
    "    Output:\n",
    "        log PDF function (N x K)\n",
    "    '''\n",
    "    def calc_log_gaussian_cluster_k(X, Mu, sigma_sq):\n",
    "        with tf.name_scope('log_gaussian_cluster'):\n",
    "            # Infer dimension of data\n",
    "            D = tf.shape(X)[1]\n",
    "\n",
    "            # Calculate Mahalanobis distance\n",
    "            ### Expand dim(X) to (N x 1 x D)\n",
    "            ### Expand dim(Mu) to (1 x K x D)\n",
    "            ### Reduce sum along the D-axis\n",
    "            with tf.name_scope('mahalanobis_dist'):\n",
    "                dist = - tf.divide(tf.reduce_sum(tf.square(tf.expand_dims(X, axis=1) \\\n",
    "                                                               - tf.expand_dims(Mu, axis=0)), axis=2),\n",
    "                                   2 * tf.transpose(sigma_sq), \\\n",
    "                                   name='mahalanobis_dist')\n",
    "                \n",
    "            # Calculate log of gaussian constant term\n",
    "            ### Transpose sigma_sq to (1 x K)\n",
    "            with tf.name_scope('log_gauss_const'):\n",
    "                log_gauss_const = - tf.multiply(tf.cast(D, tf.float32) / 2, \\\n",
    "                                                tf.log(2 * np.pi * tf.transpose(sigma_sq)),\\\n",
    "                                                name='log_gauss_const')\n",
    "\n",
    "            # Sum results\n",
    "            log_gaussian_cluster = tf.add(dist, log_gauss_const, name='log_gauss_cluster')\n",
    "\n",
    "        return log_gaussian_cluster\n",
    "    \n",
    "    '''\n",
    "    Calculate log probability cluster variable z given x, a.k.a. conditional responsibilities, gamma\n",
    "\n",
    "    Output:\n",
    "        conditional responsibilities (N x K)\n",
    "    '''\n",
    "    def calc_log_conditional_responsibilities(X, Mu, sigma_sq, log_pi):\n",
    "        with tf.name_scope('log_conditional_responsibilities'):\n",
    "            # Calculate unnormalised_log_posterior P(z|x)\n",
    "            with tf.name_scope('unnormalised_log_posterior'):\n",
    "                unnormalised_log_posterior = calc_log_gaussian_cluster_k(X, Mu, sigma_sq) + tf.transpose(log_pi)\n",
    "\n",
    "            # Return log normalised posterior / conditional responsibilities\n",
    "            with tf.name_scope('log_gamma_z'):\n",
    "                cond_resp = tf.add(- tf.reduce_logsumexp(unnormalised_log_posterior, axis=1, keep_dims=True),\\\n",
    "                                   unnormalised_log_posterior, \\\n",
    "                                   name='log_gamma_z')\n",
    "        return cond_resp\n",
    "    \n",
    "    '''\n",
    "    Calculates the negative log marginal probability, -log P(X), aka the loss function for MoG\n",
    "\n",
    "    Output:\n",
    "        - log P(X) (scalar)\n",
    "    '''\n",
    "    def calc_neg_log_marg_prob(X, Mu, sigma_sq, log_pi):\n",
    "        with tf.name_scope('loss'):\n",
    "            loss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(calc_log_gaussian_cluster_k(X, Mu, sigma_sq) \\\n",
    "                                                                 + tf.transpose(log_pi), axis=1),\\\n",
    "                                             axis=0), name='-log_P_X')\n",
    "        return loss\n",
    "    \n",
    "    '''\n",
    "    Helper function to add histogram tag to variables\n",
    "    Input:\n",
    "        var: variable to be tagged with histogram summary\n",
    "    '''\n",
    "    def _add_histogram(vars_):\n",
    "        for var in vars_:\n",
    "            tf.summary.histogram(var.op.name, var)\n",
    "    \n",
    "    #######################\n",
    "    ##  Function begins  ##\n",
    "    #######################\n",
    "    \n",
    "    # Fix TF graph seed\n",
    "    tf.set_random_seed(SEED)\n",
    "    \n",
    "    # Define computation device\n",
    "    try:\n",
    "        assert device == 'cpu' or device == 'gpu'\n",
    "    except AssertionError:\n",
    "        print 'Invalid device chosen. Please use \\'cpu\\' or \\'gpu\\''\n",
    "        quit()\n",
    "    device = '/' + device + ':0'\n",
    "    \n",
    "    with tf.device('/cpu:0'):\n",
    "        # Define placeholder\n",
    "        with tf.name_scope('placeholder'):\n",
    "            X = tf.placeholder(tf.float32, shape=[None, D], name='inputs')\n",
    "            \n",
    "        # Define parameters\n",
    "        with tf.variable_scope('parameters'):\n",
    "            Mu = tf.get_variable('cluster_centres', shape=[K, D], \\\n",
    "                                initializer=tf.truncated_normal_initializer(seed=SEED))\n",
    "            phi = tf.get_variable('latent_for_sigma_sq', shape=[K, 1], \\\n",
    "                                       initializer=tf.truncated_normal_initializer(seed=SEED))\n",
    "            psi = tf.get_variable('latent_for_pi', shape=[K, 1], \\\n",
    "                                 initializer=tf.truncated_normal_initializer(seed=SEED + 1))\n",
    "            \n",
    "            # Calculate bounded variables sigma_sq and pi\n",
    "            sigma_sq = tf.exp(phi, name='sigma_sq')\n",
    "            \n",
    "            with tf.name_scope('log_pi'):\n",
    "                log_pi = tf.transpose(tf.nn.log_softmax(tf.transpose(psi)), name='log_pi') \n",
    "        \n",
    "    with tf.device(device):\n",
    "        # Calculate conditional responsibilities\n",
    "        log_resp = calc_log_conditional_responsibilities(X, Mu, sigma_sq, log_pi)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = calc_neg_log_marg_prob(X, Mu, sigma_sq, log_pi)\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        \n",
    "        # Define optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.01, \\\n",
    "                                           beta1=0.9, beta2=0.99, epsilon=1e-5).minimize(loss)\n",
    "        \n",
    "    with tf.device('/cpu:0'):\n",
    "        # Add histogram summaries for variables of interest\n",
    "        _add_histogram([Mu, phi, psi, sigma_sq, log_pi, log_resp])\n",
    "        \n",
    "        # Merge all summaries\n",
    "        merged = tf.summary.merge_all()\n",
    "        \n",
    "    return X, Mu, sigma_sq, log_pi, log_resp, loss, optimizer, merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     3
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Runs MoG training algorithm\n",
    "'''\n",
    "def run_MoG(K_list, D, QUES_DIR, has_valid=False):\n",
    "    '''\n",
    "    If has_valid is true, subsets:\n",
    "        first 2/3 of data as training data\n",
    "        remaining 1/3 of data as validation data\n",
    "    '''\n",
    "    def subset_data(D):\n",
    "        if D == 2:\n",
    "            data = data2D\n",
    "        elif D == 100:\n",
    "            data = data100D\n",
    "        divider = data.shape[0] * 2 / 3\n",
    "        return data[:divider], data[divider:]\n",
    "    \n",
    "    #######################\n",
    "    ##  Function begins  ##\n",
    "    #######################\n",
    "    '''\n",
    "    cluster_centres: 11 x K x D\n",
    "    train_resp:        11 x N x K\n",
    "    '''\n",
    "    \n",
    "    # Assert correct value for D\n",
    "    assert D == 2 or D == 100\n",
    "    \n",
    "    # Define locally global function\n",
    "    MAX_ITER = 1000\n",
    "    CURR_TIME = '{:%b%d %H_%M_%S}'.format(datetime.datetime.now())\n",
    "    SUMMARY_DIR = CURRENT_DIR + LOG_DIR + '/MoG' + QUES_DIR + '/' + CURR_TIME\n",
    "    \n",
    "    # Create list to store run results\n",
    "    results = []\n",
    "    \n",
    "    for K in K_list:\n",
    "        # Clear any pre-defined graph\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build TensorFlow graph\n",
    "        X, Mu, sigma_sq, log_pi, resp, loss, optimizer, merged = build_MoG(K, D)\n",
    "        \n",
    "        # Select appropriate input_data\n",
    "        if has_valid:\n",
    "            input_data, valid_data = subset_data(D)\n",
    "        else:\n",
    "            input_data = data2D if D == 2 else data100D\n",
    "\n",
    "        # Create arrays to log losses, cluster_centres, cluster_variances, pi's, and responsbility indices\n",
    "        train_loss = np.array([])[:, np.newaxis]            \n",
    "        if has_valid:\n",
    "            valid_loss = np.array([])[:, np.newaxis]\n",
    "            valid_resp = np.array([])[:, np.newaxis, np.newaxis].reshape(0, valid_data.shape[0], K)\n",
    "        cluster_centres = np.array([])[:, np.newaxis, np.newaxis].reshape(0, K, D)\n",
    "        cluster_variances = np.array([])[:, np.newaxis].reshape(0, K)\n",
    "        pi = np.array([])[:, np.newaxis].reshape(0, K)\n",
    "        train_resp = np.array([])[:, np.newaxis, np.newaxis].reshape(0, input_data.shape[0], K)\n",
    "        \n",
    "        # Begin session\n",
    "        with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)) as sess:\n",
    "            # Log start time\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Create sub-directory title\n",
    "            sub_dir = '/K={},D={},valid={}'.format(K, D, has_valid)\n",
    "            \n",
    "            # Create summary writers\n",
    "            train_writer = tf.summary.FileWriter(SUMMARY_DIR + sub_dir + '/train', graph=sess.graph)\n",
    "            if has_valid:\n",
    "                valid_writer = tf.summary.FileWriter(SUMMARY_DIR + sub_dir + '/valid')\n",
    "\n",
    "            # Initialise all TensorFlow variables\n",
    "            tf.global_variables_initializer().run()\n",
    "            \n",
    "            # Define iterator\n",
    "            curr_iter = 0\n",
    "            \n",
    "            # Calculate training (and validation) loss, \n",
    "            # cluster centres and responsibility indices before any training\n",
    "            err, summaries, clusters, variances, log_pi_prob, train_indices = \\\n",
    "                sess.run([loss, merged, Mu, sigma_sq, log_pi, resp], feed_dict={X:input_data})\n",
    "            train_loss = np.append(train_loss, err)\n",
    "            train_writer.add_summary(summaries, curr_iter)\n",
    "            \n",
    "            \n",
    "            # Log validation data\n",
    "            if has_valid:\n",
    "                err, valid_indices, summaries  = sess.run([loss, resp, merged], feed_dict={X:valid_data})\n",
    "                \n",
    "                valid_loss = np.append(valid_loss, err)\n",
    "                valid_resp = np.append(valid_resp, valid_indices[np.newaxis, :, :], axis=0)\n",
    "                valid_writer.add_summary(summaries, curr_iter)\n",
    "            \n",
    "            # Log clusters and responsibility indices\n",
    "            cluster_centres = np.append(cluster_centres, clusters[np.newaxis,:,:], axis=0)\n",
    "            cluster_variances = np.append(cluster_variances, np.transpose(variances), axis=0)\n",
    "            pi = np.append(pi, np.transpose(np.exp(log_pi_prob)), axis=0)\n",
    "            \n",
    "            train_resp = np.append(train_resp, train_indices[np.newaxis,:,:], axis=0)\n",
    "            \n",
    "            # Begin training\n",
    "            while curr_iter < MAX_ITER:                \n",
    "                # Train graph\n",
    "                _, err, summaries = sess.run([optimizer, loss, merged], feed_dict={X:input_data})\n",
    "            \n",
    "                \n",
    "                # Add training loss\n",
    "                train_writer.add_summary(summaries, curr_iter + 1)\n",
    "                train_loss = np.append(train_loss, err)\n",
    "\n",
    "                # Log validation loss\n",
    "                if has_valid:\n",
    "                    err, valid_indices, summaries = sess.run([loss, resp, merged], feed_dict={X:valid_data})\n",
    "                    valid_loss = np.append(valid_loss, err)\n",
    "                    valid_resp = np.append(valid_resp, valid_indices[np.newaxis, :, :], axis=0)\n",
    "                    valid_writer.add_summary(summaries, curr_iter)\n",
    "                \n",
    "                # Log responsibility indices and cluster centres every 10% of maximum iteration\n",
    "                if ((float(curr_iter) + 1) * 100 / MAX_ITER) % 10 == 0:\n",
    "                    clusters, variances, log_pi_prob, train_indices = \\\n",
    "                        sess.run([Mu, sigma_sq, log_pi, resp], feed_dict={X:input_data})\n",
    "                    \n",
    "                    cluster_centres = np.append(cluster_centres, clusters[np.newaxis, :, :], axis=0)\n",
    "                    cluster_variances = np.append(cluster_variances, np.transpose(variances), axis=0)\n",
    "                    pi = np.append(pi, np.transpose(np.exp(log_pi_prob)), axis=0)\n",
    "                    \n",
    "                    train_resp = np.append(train_resp, train_indices[np.newaxis,:,:], axis=0)\n",
    "                \n",
    "                # Post training progress to user, every 100 iterations\n",
    "                if curr_iter % 100 == 99:\n",
    "                    if not has_valid:\n",
    "                        print 'iter: {:3d}, train_loss: {:3.1f}'.format(curr_iter, train_loss[curr_iter])\n",
    "                    else:\n",
    "                        print 'iter: {:3d}, train_loss: {:3.1f}, valid_loss: {:3.1f}'\\\n",
    "                                .format(curr_iter + 1, train_loss[curr_iter], valid_loss[curr_iter])\n",
    "                \n",
    "                curr_iter += 1\n",
    "            \n",
    "            # End of while loop\n",
    "            print 'Max iteration reached'\n",
    "            train_writer.close()\n",
    "            if has_valid:\n",
    "                valid_writer.close()\n",
    "            \n",
    "            if not has_valid:\n",
    "                results.append(\n",
    "                    {\n",
    "                        'K': K,\n",
    "                        'train_loss': train_loss,\n",
    "                        'cluster_centres': cluster_centres,\n",
    "                        'cluster_variances': cluster_variances,\n",
    "                        'cluster_prob': pi,\n",
    "                        'train_resp': train_resp,\n",
    "                        'time_of_run': '{:%b%d %H_%M_%S}'.format(datetime.datetime.now())\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                results.append(\n",
    "                {\n",
    "                    'K': K,\n",
    "                    'train_loss': train_loss,\n",
    "                    'valid_loss': valid_loss,\n",
    "                    'cluster_centres': cluster_centres,\n",
    "                    'cluster_variances': cluster_variances,\n",
    "                    'cluster_prob': pi,\n",
    "                    'train_resp': train_resp,\n",
    "                    'valid_resp': valid_resp,\n",
    "                    'time_of_run': '{:%b%d %H_%M_%S}'.format(datetime.datetime.now())\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # TODO calculate convergence\n",
    "            if not has_valid:\n",
    "                print 'K: {:3d}, train loss: {:3.1f}, duration: {:3.1f}s\\n'\\\n",
    "                        .format(K, train_loss[-1], time.time() - start_time)\n",
    "            else:\n",
    "                print 'K: {:3d}, train loss: {:3.1f}, valid loss: {:3.1f}, duration: {:3.1f}s\\n'\\\n",
    "                        .format(K, train_loss[-1], valid_loss[-1], time.time() - start_time)\n",
    "                                                                              \n",
    "    print 'RUN COMPLETED'\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [
     21,
     101,
     114,
     129,
     138,
     145,
     172,
     177,
     179,
     191
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Runs MoG training algorithm more efficiently by not saving loss values.\n",
    "    Tensorboard embedding enabled\n",
    "'''\n",
    "def run_MoG_v2(input_data, K_list, D, QUES_DIR, has_valid=False, device='cpu'):    \n",
    "    '''\n",
    "    Embed data for visualization purposes\n",
    "    '''\n",
    "    def embed_data(D, train_writer):\n",
    "        # Define input data\n",
    "        input_data = data2D if D == 2 else data100D\n",
    "        input_data_name = 'data{}D.npy'.format(D)\n",
    "        \n",
    "        # Create variable to embed\n",
    "        data_to_embed = tf.Variable(input_data, name=input_data_name, trainable=False, collections=[])\n",
    "\n",
    "        # Define projector configurations\n",
    "        config = projector.ProjectorConfig()\n",
    "        \n",
    "        # Add embedding\n",
    "        embedding = config.embeddings.add()\n",
    "        \n",
    "        # Connect tf.Variable to embedding\n",
    "        embedding.tensor_name = data_to_embed.name\n",
    "\n",
    "        # Evaluate tf.Variable\n",
    "        sess.run(data_to_embed.initializer)\n",
    "        \n",
    "        # Create save checkpoint\n",
    "        saver = tf.train.Saver([data_to_embed])\n",
    "        saver.save(sess, SUMMARY_DIR + sub_dir + '/train/model.ckpt', MAX_ITER)\n",
    "\n",
    "        # Write projector_config.pbtxt in LOG_DIR\n",
    "        projector.visualize_embeddings(train_writer, config)\n",
    "    \n",
    "    #######################\n",
    "    ##  Function begins  ##\n",
    "    #######################\n",
    "    '''\n",
    "    cluster_centres: 11 x K x D\n",
    "    train_resp:        11 x N x K\n",
    "    '''\n",
    "    \n",
    "    # Assert correct value for D\n",
    "    assert D == 2 or D == 100\n",
    "    \n",
    "    # Define locally global function\n",
    "    MAX_ITER = 1500\n",
    "    CURR_TIME = '{:%b%d %H_%M_%S}'.format(datetime.datetime.now())\n",
    "    SUMMARY_DIR = CURRENT_DIR + LOG_DIR + '/MoG' + QUES_DIR + '/' + CURR_TIME\n",
    "    \n",
    "    # Create list to store run results\n",
    "    results = []\n",
    "    \n",
    "    for K in K_list:\n",
    "        # Clear any pre-defined graph\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build TensorFlow graph\n",
    "        X, Mu, sigma_sq, log_pi, log_resp, loss, optimizer, merged = build_MoG(K, D, device)\n",
    "\n",
    "        # Create arrays to log cluster_centres, cluster_variances, pi's, and responsbility indices\n",
    "        train_loss = np.array([])[:, np.newaxis]\n",
    "        if has_valid:\n",
    "            valid_loss = np.array([])[:, np.newaxis]\n",
    "            valid_resp = np.array([])[:, np.newaxis, np.newaxis].reshape(0, valid_data.shape[0], K)\n",
    "        cluster_centres = np.array([])[:, np.newaxis, np.newaxis].reshape(0, K, D)\n",
    "        cluster_variances = np.array([])[:, np.newaxis].reshape(0, K)\n",
    "        cluster_pi = np.array([])[:, np.newaxis].reshape(0, K)\n",
    "        train_resp = np.array([])[:, np.newaxis, np.newaxis].reshape(0, input_data.shape[0], K)\n",
    "        \n",
    "        # Begin session\n",
    "        with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)) as sess:\n",
    "            # Log start time\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Create sub-directory title\n",
    "            sub_dir = '/K={},D={},valid={}'.format(K, D, has_valid)\n",
    "            \n",
    "            # Create summary writers\n",
    "#             train_writer = tf.summary.FileWriter(SUMMARY_DIR + sub_dir + '/train', graph=sess.graph)\n",
    "#             if has_valid:\n",
    "#                 valid_writer = tf.summary.FileWriter(SUMMARY_DIR + sub_dir + '/valid')\n",
    "                \n",
    "            # Initialise all TensorFlow variables\n",
    "            tf.global_variables_initializer().run()\n",
    "            \n",
    "            # Define iterator\n",
    "            curr_iter = 0\n",
    "            \n",
    "            # Calculate training (and validation) loss, \n",
    "            # cluster centres and responsibility indices before any training\n",
    "            err, summaries, clusters, variances, log_prior_pi, log_train_indices = \\\n",
    "                sess.run([loss, merged, Mu, sigma_sq, log_pi, log_resp], feed_dict={X:input_data})\n",
    "            train_loss = np.append(train_loss, err)\n",
    "#             train_writer.add_summary(summaries, curr_iter)\n",
    "            \n",
    "            # Log clusters and responsibility indices\n",
    "            cluster_centres = np.append(cluster_centres, clusters[np.newaxis,:,:], axis=0)\n",
    "            cluster_variances = np.append(cluster_variances, np.transpose(variances), axis=0)\n",
    "            cluster_pi = np.append(cluster_pi, np.exp(np.transpose(log_prior_pi)), axis=0)\n",
    "\n",
    "            train_resp = np.append(train_resp, np.exp(log_train_indices)[np.newaxis,:,:], axis=0)\n",
    "            \n",
    "            # Log validation data\n",
    "            if has_valid:\n",
    "                err, log_valid_indices, summaries  = sess.run([loss, log_resp, merged], feed_dict={X:valid_data})\n",
    "                \n",
    "                valid_loss = np.append(valid_loss, err)\n",
    "                valid_resp = np.append(valid_resp, np.exp(log_valid_indices)[np.newaxis, :, :], axis=0)\n",
    "#                 valid_writer.add_summary(summaries, curr_iter)\n",
    "            \n",
    "            # Begin training\n",
    "            while curr_iter < MAX_ITER:                \n",
    "                # Train graph\n",
    "                _, summaries, err = sess.run([optimizer, merged, loss], feed_dict={X:input_data})\n",
    "                \n",
    "                # Add training loss\n",
    "                train_loss = np.append(train_loss, err)\n",
    "#                 train_writer.add_summary(summaries, curr_iter + 1)\n",
    "\n",
    "                # Log validation loss\n",
    "                if has_valid:\n",
    "                    summaries, err = sess.run([merged, loss], feed_dict={X:valid_data})\n",
    "                    \n",
    "                    valid_loss = np.append(valid_loss, err)\n",
    "#                     valid_writer.add_summary(summaries, curr_iter)\n",
    "                \n",
    "                # Log responsibility indices and cluster centres every 10% of maximum iteration\n",
    "                if ((float(curr_iter) + 1) * 100 / MAX_ITER) % 10 == 0:\n",
    "                    clusters, variances, log_prior_pi, log_train_indices = \\\n",
    "                        sess.run([Mu, sigma_sq, log_pi, log_resp], feed_dict={X:input_data})\n",
    "                    \n",
    "                    cluster_centres = np.append(cluster_centres, clusters[np.newaxis, :, :], axis=0)\n",
    "                    cluster_variances = np.append(cluster_variances, np.transpose(variances), axis=0)\n",
    "                    cluster_pi = np.append(cluster_pi, np.exp(np.transpose(log_prior_pi)), axis=0)\n",
    "                    \n",
    "                    train_resp = np.append(train_resp, np.exp(log_train_indices)[np.newaxis,:,:], axis=0)\n",
    "                    \n",
    "                    if has_valid:\n",
    "                        log_valid_indices = sess.run(log_resp, feed_dict={X:valid_data})\n",
    "                        valid_resp = np.append(valid_resp, np.exp(log_valid_indices)[np.newaxis, :, :], axis=0)\n",
    "                \n",
    "                # Post training progress to user, every 100 iterations\n",
    "                if curr_iter % 100 == 99:\n",
    "                    print 'iter: {:3d}'.format(curr_iter + 1)\n",
    "                \n",
    "                curr_iter += 1\n",
    "            \n",
    "            # End of while loop\n",
    "            print 'Max iteration reached'\n",
    "            \n",
    "            # Embed data\n",
    "#             embed_data(D, train_writer)\n",
    "            \n",
    "            # Close writers\n",
    "#             train_writer.close()\n",
    "#             if has_valid:\n",
    "#                 valid_writer.close()\n",
    "            \n",
    "            if not has_valid:\n",
    "                results.append(\n",
    "                    {\n",
    "                        'K': K,\n",
    "                        'train_loss': train_loss,\n",
    "                        'cluster_centres': cluster_centres,\n",
    "                        'cluster_variances': cluster_variances,\n",
    "                        'cluster_pi': cluster_pi,\n",
    "                        'train_resp': train_resp,\n",
    "                        'time_of_run': '{:%b%d %H_%M_%S}'.format(datetime.datetime.now())\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                results.append(\n",
    "                {\n",
    "                    'K': K,\n",
    "                    'train_loss': train_loss,\n",
    "                    'valid_loss': valid_loss,\n",
    "                    'cluster_centres': cluster_centres,\n",
    "                    'cluster_variances': cluster_variances,\n",
    "                    'cluster_pi': cluster_pi,\n",
    "                    'train_resp': train_resp,\n",
    "                    'valid_resp': valid_resp,\n",
    "                    'time_of_run': '{:%b%d %H_%M_%S}'.format(datetime.datetime.now())\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # TODO calculate convergence\n",
    "            print 'K: {:3d}, duration: {:3.1f}s\\n'.format(K, time.time() - start_time)\n",
    "                                                                              \n",
    "    print 'RUN COMPLETED'\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 100\n",
      "iter: 200\n",
      "iter: 300\n",
      "iter: 400\n",
      "iter: 500\n",
      "iter: 600\n",
      "iter: 700\n",
      "iter: 800\n",
      "iter: 900\n",
      "iter: 1000\n",
      "iter: 1100\n",
      "iter: 1200\n",
      "iter: 1300\n",
      "iter: 1400\n",
      "iter: 1500\n",
      "Max iteration reached\n",
      "K:   4, duration: 4.2s\n",
      "\n",
      "RUN COMPLETED\n",
      "Stored 'result' (list)\n"
     ]
    }
   ],
   "source": [
    "result = run_MoG_v2(input_data=target, K_list=[4], D=2, QUES_DIR=\"./\", device='gpu')\n",
    "%store result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Section 3.2.2.2: MoG on $\\textit{data2D.npy}$ without validation $(K = 3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate bar chart for cluster assignment $\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Generate a bar chart for each model showing percentage of data points belong to each cluster\n",
    "'''\n",
    "def cluster_assignment_IGraph(results, is_MoG, D, question_name):\n",
    "    assert D == 2 or D == 100\n",
    "    \n",
    "    # Define colour list as per Plotly's default colour list\n",
    "    colour_list = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "    \n",
    "    # Define empty figure\n",
    "    figure = {\n",
    "        'data': [],\n",
    "        'layout': {}\n",
    "    }\n",
    "    \n",
    "    # Define data to plot\n",
    "    for i, result in enumerate(results):\n",
    "        for k in range(result['K']):\n",
    "            trace = go.Bar(\n",
    "                x = [i + 1],\n",
    "                y = [result['cluster_pi'][-1][k]] if is_MoG == True else [result['composition'][k]],\n",
    "                marker = {'color': colour_list[k]},\n",
    "                name = 'Cluster {}'.format(k + 1)\n",
    "            )\n",
    "            figure['data'].append(trace)\n",
    "    \n",
    "    # Define layout\n",
    "    figure['layout'] = {\n",
    "        'title': 'Percentage of data points assigned to each {} cluster on data{}D.npy'\\\n",
    "            .format('MoG' if is_MoG == True else 'K-means', D),\n",
    "        'xaxis': {'title': 'Number of clusters, K'},\n",
    "        'yaxis': {'title': 'Assignment to cluster, %'},\n",
    "        'barmode': 'stack',\n",
    "        'showlegend': False\n",
    "    }\n",
    "    \n",
    "    # Generate plot\n",
    "    py.iplot(figure, \\\n",
    "             filename='/ECE521: A3/Q2: Mixture of Gaussians/Q{}_assignment_bar_chart_{}D_{}'\\\n",
    "                 .format(question_name, D, 'MoG' if is_MoG is True else 'K-means'),\\\n",
    "             sharing='private')\n",
    "    \n",
    "    return pyo.iplot(figure)\n",
    "\n",
    "cluster_assignment_IGraph(results_2_2_3, is_MoG=True, D=2, question_name='2.3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     10
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Final result by colouring data points by clusters generated by Mixture of Gaussian algorithm\n",
    "Input:\n",
    "    result:           MoG training result with validation\n",
    "Notes:\n",
    "    cluster_centres:  coordinates of cluster centres (K x D)\n",
    "    cluter_variances: cluster variances (K)\n",
    "    train_resp:       training responsibility indices for each run of K ((N*2/3) x K)\n",
    "    valid_resp:       validation responsibility indices for each run of K ((N/3) x K)\n",
    "'''\n",
    "def visualise_MoG_clusters(result):\n",
    "    '''\n",
    "    Convert hex values of type string to RGB of type int\n",
    "    Input:\n",
    "        colour_list: numpy array of type string (numColour x 1)\n",
    "    Output:\n",
    "        RGB: RGB component of type int (numColour x 3)\n",
    "    '''\n",
    "    def _hex_to_rgb(colour_list):\n",
    "        RGB = np.array([])[np.newaxis,:].reshape(0,3)\n",
    "        # Split hex values into R, G, B components\n",
    "        # Convert components to int and store in RGB array\n",
    "        for colour in colour_list:\n",
    "            RGB = np.append(RGB, np.array([int(colour[1:3], 16), \\\n",
    "                                           int(colour[3:5], 16), \\\n",
    "                                           int(colour[5:7], 16)]).reshape(1, 3), axis=0)\n",
    "        return RGB\n",
    "\n",
    "    '''\n",
    "    Convert RGB of type int to hex string of format '#xxxxxx'\n",
    "    Input:\n",
    "        RGB: RGB component of type int (N x 3)\n",
    "    Output:\n",
    "        hex_colours: (N x 1)\n",
    "    '''\n",
    "    def _rgb_to_hex(RGB):\n",
    "        hex_colours = np.array([])\n",
    "        # Convert RGB ints to a single hex string\n",
    "        RGB = RGB.astype(int)\n",
    "        for colour in RGB:\n",
    "            hex_colours = np.append(hex_colours, '#{:02X}{:02X}{:02X}'.format(colour[0], colour[1], colour[2]))\n",
    "        return hex_colours\n",
    "\n",
    "    '''\n",
    "    Return the 'average' colour based on Plotly's default colour list and responsibility index\n",
    "    Input:\n",
    "        idx: responsibility index (N x K)\n",
    "    Output:\n",
    "        average_colour (N x 1)\n",
    "    '''\n",
    "    def get_colour_gradient(resp):\n",
    "        # Assert error if there are more colours than available colours\n",
    "        N = resp.shape[0]\n",
    "        K = resp.shape[1]\n",
    "        try:\n",
    "            assert K <= colour_list.shape\n",
    "        except AssertionError:\n",
    "            print 'Not enough colours to colour all K clusters. Consider increasing number of colours in colour_list.'\n",
    "\n",
    "        # Matrix multiply resp (N x K) and RGB-ed colour_list (K x 3) to obtain 'average' colour\n",
    "        # Multiply max resp to whiten less certain data points\n",
    "        # assigned_colour = np.matmul(resp, _hex_to_rgb(colour_list[:K]))\n",
    "        assigned_colour = np.matmul(np.eye(K, dtype='int')[np.argmax(resp, axis=1)], _hex_to_rgb(colour_list[:K]))\n",
    "        white_layer = np.repeat(255, N * 3).reshape(N, 3)\n",
    "        \n",
    "        # Append white_layer to assigned_colour on axis=2\n",
    "        # pre_whitened (N x K x 2)\n",
    "        pre_whitened = np.append(assigned_colour[:,:,np.newaxis], white_layer[:,:,np.newaxis], axis=2)\n",
    "\n",
    "        # Create weights (N x 2)\n",
    "        # Second layer takes the converse of the maximum responsibility (N x 1)\n",
    "        weights = np.append(np.ones(N)[:,np.newaxis], 1 - np.amax(resp, axis=1)[:, np.newaxis], axis=1)\n",
    "\n",
    "        # Conform shape of weights to shape of pre_whitened\n",
    "        weights = np.transpose(np.tile(weights, (3, 1, 1)), (1, 0, 2))\n",
    "\n",
    "        # Perform weighted-average to colours\n",
    "        whitened_colour = np.average(pre_whitened, weights=weights, axis=2)\n",
    "\n",
    "        # Return matrix of colour in hex form\n",
    "        return _rgb_to_hex(whitened_colour)\n",
    "\n",
    "    '''\n",
    "    Create x- and y-coordinates for ellipses for each cluster\n",
    "    Assummptions:\n",
    "        Joint independence and equal marginal variances\n",
    "        Dimension of data point is 2\n",
    "    Returns:\n",
    "        ellipse: x- and y-coordinates for K ellipses (N x K x D)\n",
    "    '''\n",
    "    def calc_ellipse_coordinates(centres, variances):\n",
    "        # Create trace for region to encompass 95% of the points (using Chi-squared critical value)\n",
    "        # Assuming joint independence and equal marginal variances\n",
    "        \n",
    "        # Chi-squared with df 2 and alpha=5%\n",
    "        crit_val = 5.991\n",
    "        \n",
    "        # Calculate axes length\n",
    "        axis_lengths = np.sqrt(variances * crit_val)\n",
    "        \n",
    "        # Calculate coordinates to trace ellipse\n",
    "        t = np.arange(-np.pi, np.pi + np.pi / 50, np.pi / 50) # Parameter\n",
    "        x = np.transpose(centres[:,0][:, np.newaxis]) + axis_lengths * np.cos(t)[:, np.newaxis]\n",
    "        y = np.transpose(centres[:,1][:, np.newaxis]) + axis_lengths * np.sin(t)[:, np.newaxis]\n",
    "        \n",
    "        # Stack x- and y-coordinates along axis=2\n",
    "        ellipse = np.stack([x, y], axis=2)\n",
    "        \n",
    "        return ellipse\n",
    "    \n",
    "    #######################\n",
    "    ##  Function begins  ##\n",
    "    #######################\n",
    "    \n",
    "    # Define K and divider between training and validation data\n",
    "    K = result['K']\n",
    "    divider = data2D.shape[0] * 2 / 3 # Anything before K is part of the training data. Anything after is part of validation data\n",
    "    \n",
    "    # Store cluster parameters and responsibility indices\n",
    "    centres = result['cluster_centres'][-1]\n",
    "    variances = result['cluster_variances'][-1]\n",
    "    train_resp = result['train_resp'][-1]\n",
    "    valid_resp = result['valid_resp'][-1]\n",
    "    \n",
    "    # Create ellipse coordinates\n",
    "    ellipse = calc_ellipse_coordinates(centres, variances)\n",
    "    \n",
    "    # Define colour list as per Plotly's default colour list\n",
    "    colour_list = np.array(['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b'])\n",
    "    \n",
    "    # Define blank figure\n",
    "    figure = {\n",
    "        'data': [],\n",
    "        'layout': {}\n",
    "    }\n",
    "    \n",
    "    # Create trace for training data points\n",
    "    # Create trace for validation data points\n",
    "    valid_data_trace = {\n",
    "        'x': data2D[divider:][:,0],\n",
    "        'y': data2D[divider:][:,1],\n",
    "        'mode': 'markers',\n",
    "        'hoverinfo': 'none',\n",
    "        'marker': {\n",
    "            'size': 4,\n",
    "            'color': colour_list[np.argmax(valid_resp, axis=1)] #get_colour_gradient(valid_resp)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Append data traces\n",
    "    figure['data'].append(valid_data_trace)\n",
    "    \n",
    "    for k in range(K):\n",
    "        # Create trace for cluster centres\n",
    "        centre_trace = {\n",
    "            'x': np.round([centres[k][0]], 3),\n",
    "            'y': np.round([centres[k][1]], 3),\n",
    "            'name': 'Cluster {}'.format(k + 1),\n",
    "            'mode': 'markers',\n",
    "            'marker': {\n",
    "                    'size': 12,\n",
    "                    'symbol': 'diamond',\n",
    "                    'color': colour_list[k],\n",
    "                    'line': {'width': 3}\n",
    "                }   \n",
    "        }\n",
    "\n",
    "        # Create trace for region encompassing 95% of data points\n",
    "        variance_trace = {\n",
    "            'x': ellipse[:,k,:][:,0],\n",
    "            'y': ellipse[:,k,:][:,1],\n",
    "            'hoverinfo': 'none',\n",
    "            'mode': 'lines',\n",
    "            'name': 'Cluster {}'.format(k + 1),\n",
    "            'marker': {\n",
    "                'color': colour_list[k]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add cluster trace\n",
    "        for trace in [centre_trace, variance_trace]:\n",
    "            figure['data'].append(trace)\n",
    "\n",
    "    # Generate figure layout\n",
    "    figure['layout'] = go.Layout(\n",
    "        width = 900,\n",
    "        height = 900,\n",
    "        showlegend = False,\n",
    "        title = 'MoG Clustering Visualisation (K = {})'.format(K),\n",
    "        xaxis = {'range': [-4, 4], 'autorange': False},\n",
    "        yaxis = {'range': [-5, 2], 'autorange': False}\n",
    "    )\n",
    "    \n",
    "    # Upload graph to cloud\n",
    "    py.iplot(figure, \\\n",
    "             filename='/ECE521: A3/Q2: Mixture of Gaussians/Q2.3_MoG_clusters_K={}'.format(K), \\\n",
    "             sharing='private')\n",
    "    \n",
    "    return pyo.iplot(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig2_2_3 = []\n",
    "for result in results_2_2_3:\n",
    "    fig2_2_3.append(visualise_MoG_clusters(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create GIFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     10
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Creates snapshots of animated plots, with data points coloured by clusters\n",
    "Input:\n",
    "    result:           MoG training result with validation\n",
    "Notes:\n",
    "    cluster_centres:  coordinates of cluster centres (11 x K x D)\n",
    "    cluter_variances: cluster variances (11 x K)\n",
    "    train_resp:       training responsibility indices for each run of K (11 x (N*2/3) x K)\n",
    "    valid_resp:       validation responsibility indices for each run of K (11 x (N/3) x K)\n",
    "'''\n",
    "def generate_gif_images(result):\n",
    "    '''\n",
    "    Convert hex values of type string to RGB of type int\n",
    "    Input:\n",
    "        colour_list: numpy array of type string (numColour x 1)\n",
    "    Output:\n",
    "        RGB: RGB component of type int (numColour x 3)\n",
    "    '''\n",
    "    def _hex_to_rgb(colour_list):\n",
    "        RGB = np.array([])[np.newaxis,:].reshape(0,3)\n",
    "        # Split hex values into R, G, B components\n",
    "        # Convert components to int and store in RGB array\n",
    "        for colour in colour_list:\n",
    "            RGB = np.append(RGB, np.array([int(colour[1:3], 16), \\\n",
    "                                           int(colour[3:5], 16), \\\n",
    "                                           int(colour[5:7], 16)]).reshape(1, 3), axis=0)\n",
    "        return RGB\n",
    "\n",
    "    '''\n",
    "    Convert RGB of type int to hex string of format '#xxxxxx'\n",
    "    Input:\n",
    "        RGB: RGB component of type int (N x 3)\n",
    "    Output:\n",
    "        hex_colours: (N x 1)\n",
    "    '''\n",
    "    def _rgb_to_hex(RGB):\n",
    "        hex_colours = np.array([])\n",
    "        # Convert RGB ints to a single hex string\n",
    "        RGB = RGB.astype(int)\n",
    "        for colour in RGB:\n",
    "            hex_colours = np.append(hex_colours, '#{:02X}{:02X}{:02X}'.format(colour[0], colour[1], colour[2]))\n",
    "        return hex_colours\n",
    "\n",
    "    '''\n",
    "    Return the 'average' colour based on Plotly's default colour list and responsibility index\n",
    "    Input:\n",
    "        idx: responsibility index (N x K)\n",
    "    Output:\n",
    "        average_colour (N x 1)\n",
    "    '''\n",
    "    def get_colour_gradient(resp):\n",
    "        # Assert error if there are more colours than available colours\n",
    "        N = resp.shape[0]\n",
    "        K = resp.shape[1]\n",
    "        try:\n",
    "            assert K <= colour_list.shape\n",
    "        except AssertionError:\n",
    "            print 'Not enough colours to colour all K clusters. Consider increasing number of colours in colour_list.'\n",
    "\n",
    "        # Matrix multiply resp (N x K) and RGB-ed colour_list (K x 3) to obtain 'average' colour\n",
    "        # Multiply max resp to whiten less certain data points\n",
    "        # assigned_colour = np.matmul(resp, _hex_to_rgb(colour_list[:K]))\n",
    "        assigned_colour = np.matmul(np.eye(K, dtype='int')[np.argmax(resp, axis=1)], _hex_to_rgb(colour_list[:K]))\n",
    "        white_layer = np.repeat(255, N * 3).reshape(N, 3)\n",
    "\n",
    "        # Append white_layer to assigned_colour on axis=2\n",
    "        # pre_whitened (N x K x 2)\n",
    "        pre_whitened = np.append(assigned_colour[:,:,np.newaxis], white_layer[:,:,np.newaxis], axis=2)\n",
    "\n",
    "        # Create weights (N x 2)\n",
    "        # Second layer takes the converse of the maximum responsibility (N x 1)\n",
    "        weights = np.append(np.ones(N)[:,np.newaxis], 1 - np.amax(resp, axis=1)[:, np.newaxis], axis=1)\n",
    "\n",
    "        # Conform shape of weights to shape of pre_whitened\n",
    "        weights = np.transpose(np.tile(weights, (3, 1, 1)), (1, 0, 2))\n",
    "\n",
    "        # Perform weighted-average to colours\n",
    "        whitened_colour = np.average(pre_whitened, weights=weights, axis=2)\n",
    "\n",
    "        # Return matrix of colour in hex form\n",
    "        return _rgb_to_hex(whitened_colour)\n",
    "\n",
    "    '''\n",
    "    Create x- and y-coordinates for ellipses for each cluster\n",
    "    Assummptions:\n",
    "        Joint independence and equal marginal variances\n",
    "        Dimension of data point is 2\n",
    "    Returns:\n",
    "        ellipse: x- and y-coordinates for K ellipses (N x K x D)\n",
    "    '''\n",
    "    def calc_ellipse_coordinates(centres, variances):\n",
    "        # Create trace for region to encompass 95% of the points (using Chi-squared critical value)\n",
    "        # Assuming joint independence and equal marginal variances\n",
    "        \n",
    "        # Chi-squared with df 2 and alpha=5%\n",
    "        crit_val = 5.991 \n",
    "        \n",
    "        # Calculate axes length\n",
    "        axis_lengths = np.sqrt(variances * crit_val)\n",
    "        \n",
    "        # Calculate coordinates to trace ellipse\n",
    "        t = np.arange(-np.pi, np.pi + np.pi / 50, np.pi / 50) # Parameter\n",
    "        x = np.transpose(centres[:,0][:, np.newaxis]) + axis_lengths * np.cos(t)[:, np.newaxis]\n",
    "        y = np.transpose(centres[:,1][:, np.newaxis]) + axis_lengths * np.sin(t)[:, np.newaxis]\n",
    "        \n",
    "        # Stack x- and y-coordinates along axis=2\n",
    "        ellipse = np.stack([x, y], axis=2)\n",
    "        \n",
    "        return ellipse\n",
    "    \n",
    "    #######################\n",
    "    ##  Function begins  ##\n",
    "    #######################\n",
    "    \n",
    "    # Define K and divider between training and validation data\n",
    "    K = result['K']\n",
    "    divider = data2D.shape[0] * 2 / 3 # Anything before K is part of the training data. Anything after is part of validation data\n",
    "    \n",
    "    # Store cluster parameters and responsibility indices\n",
    "    centres = result['cluster_centres']\n",
    "    variances = result['cluster_variances']\n",
    "    train_resp = result['train_resp']\n",
    "    valid_resp = result['valid_resp']\n",
    "    \n",
    "    # Define colour list as per Plotly's default colour list\n",
    "    colour_list = np.array(['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b'])\n",
    "    \n",
    "    # Define blank figure\n",
    "    figure = {\n",
    "        'data': [],\n",
    "        'layout': {}\n",
    "    }\n",
    "    \n",
    "    # Create layout\n",
    "    figure['layout'] = {\n",
    "        'width': 900,\n",
    "        'height': 900,\n",
    "        'xaxis': {'range': [-4, 4], 'autorange': False},\n",
    "        'yaxis': {'range': [-5, 2], 'autorange': False},\n",
    "        'title': 'MoG Clutering Visualisation (K = {})'.format(K),\n",
    "        'showlegend': False\n",
    "    }\n",
    "    \n",
    "    # Define slider step\n",
    "    slider_steps = []\n",
    "    \n",
    "    # Define slider ticker labels\n",
    "    slider_values = ['0%', '10%', '20%', '30%', '40%', '50%', '60%', '70%', '80%', '90%', '100%']\n",
    "    \n",
    "    for i in range(11):\n",
    "        slider_step = {\n",
    "            'args': [\n",
    "                [slider_values[i]],\n",
    "                {'frame': {'duration': 300, 'redraw': False},\n",
    "                 'mode': 'immediate',\n",
    "                 'transition': {'duration': 300}}\n",
    "            ],\n",
    "            'label': slider_values[i],\n",
    "            'method': 'animate'\n",
    "        }\n",
    "\n",
    "        # Append slider stes to slider dictionary\n",
    "        slider_steps.append(slider_step)\n",
    "    \n",
    "    # Create snapshots\n",
    "    for i in range(11):\n",
    "        # Clears figure data for new snapshot\n",
    "        figure['data'] = []\n",
    "\n",
    "        # Create trace for validation data points\n",
    "        valid_data_trace = {\n",
    "            'x': data2D[divider:][:,0],\n",
    "            'y': data2D[divider:][:,1],\n",
    "            'mode': 'markers',\n",
    "            'hoverinfo': 'none',\n",
    "            'marker': {\n",
    "                'size': 4,\n",
    "                'color': get_colour_gradient(valid_resp[i])\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Append data traces\n",
    "        figure['data'].append(valid_data_trace)\n",
    "\n",
    "        for k in range(K):\n",
    "            # Create trace for cluster centres\n",
    "            centre_trace = {\n",
    "                'x': np.round([centres[i][k][0]], 3),\n",
    "                'y': np.round([centres[i][k][1]], 3),\n",
    "                'name': 'Cluster {}'.format(k + 1),\n",
    "                'mode': 'markers',\n",
    "                'marker': {\n",
    "                        'size': 12,\n",
    "                        'symbol': 'diamond',\n",
    "                        'color': colour_list[k],\n",
    "                        'line': {'width': 3}\n",
    "                    }   \n",
    "            }\n",
    "\n",
    "            # Create ellipse coordinates\n",
    "            ellipse = calc_ellipse_coordinates(centres[i], variances[i])\n",
    "    \n",
    "            # Create trace for region encompassing 95% of data points\n",
    "            variance_trace = {\n",
    "                'x': ellipse[:,k,:][:,0],\n",
    "                'y': ellipse[:,k,:][:,1],\n",
    "                'hoverinfo': 'none',\n",
    "                'mode': 'lines',\n",
    "                'name': 'Cluster {}'.format(k + 1),\n",
    "                'marker': {\n",
    "                    'color': colour_list[k]\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Add cluster trace\n",
    "            for trace in [centre_trace, variance_trace]:\n",
    "                figure['data'].append(trace)\n",
    "\n",
    "        # Define slider dictionary\n",
    "        slider_dict = {\n",
    "            'active': i, # Slider knob's relative starting location\n",
    "            'pad': {'b': 10, 't': 50}, # Bottom and top padding\n",
    "            'len': 1, # Slider length\n",
    "            'x': 0, # Slider x-position\n",
    "            'y': 0, # Slider y-position\n",
    "            'yanchor': 'top', \n",
    "            'xanchor': 'left',\n",
    "            'currentvalue': { # Displays current value selected by slider\n",
    "                'font': {'size': 20},\n",
    "                'prefix': 'Training: ',\n",
    "                'visible': True,\n",
    "                'xanchor': 'right'\n",
    "            },\n",
    "            'transition': {'duration': 300, 'easing': 'cubic-in-out'},\n",
    "            'steps': slider_steps\n",
    "        }\n",
    "    \n",
    "        # Add sliders to layout\n",
    "        figure['layout']['sliders'] = [slider_dict]\n",
    "        \n",
    "        # Save snapshots locally\n",
    "        py.plotly.image.save_as(figure, filename='Q2.2.3_K={0}_gif_{1:02d}.png'.format(K, i), \\\n",
    "                                width=900, height=900, scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for result in results_2_2_3:\n",
    "#     generate_gif_images(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
